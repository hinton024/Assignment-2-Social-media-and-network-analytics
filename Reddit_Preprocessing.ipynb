{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db231fa8",
   "metadata": {},
   "source": [
    "### Importing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bf1c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import nltk\n",
    "from langdetect import detect, LangDetectException\n",
    "import contractions\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818aebd1",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "#### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05975ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not under David Beckhams watch</td>\n",
       "      <td>65534</td>\n",
       "      <td>2023-10-06 17:16:05</td>\n",
       "      <td>https://v.redd.it/0mmzjpgdyisb1</td>\n",
       "      <td>2042</td>\n",
       "      <td>[Please report rule breaking posts, such as:\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>David Beckham is not letting this go...</td>\n",
       "      <td>29482</td>\n",
       "      <td>2024-01-02 14:44:45</td>\n",
       "      <td>https://i.redd.it/dowdawvo7y9c1.jpeg</td>\n",
       "      <td>982</td>\n",
       "      <td>[Welcome to r/popculturechat! ☺️\\n\\nAs a proud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David and Victoria Beckham finally addressing ...</td>\n",
       "      <td>800</td>\n",
       "      <td>2023-10-05 20:46:41</td>\n",
       "      <td>https://v.redd.it/ahnysog1vcsb1</td>\n",
       "      <td>261</td>\n",
       "      <td>[I think if they hadn’t brought this up in a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Victoria Beckham shares photo of husband David...</td>\n",
       "      <td>2687</td>\n",
       "      <td>2023-12-16 10:45:59</td>\n",
       "      <td>https://i.redd.it/nd35a08lpj6c1.jpeg</td>\n",
       "      <td>298</td>\n",
       "      <td>[Welcome to r/popculturechat! ☺️\\n\\nAs a proud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Micah Richards on David Beckham</td>\n",
       "      <td>5650</td>\n",
       "      <td>2024-09-19 18:27:36</td>\n",
       "      <td>https://v.redd.it/7o6n7df28qpd1</td>\n",
       "      <td>256</td>\n",
       "      <td>[**Mirrors / Alternative Angles**\\n  \\n\\n*I am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score  \\\n",
       "0                     Not under David Beckhams watch  65534   \n",
       "1            David Beckham is not letting this go...  29482   \n",
       "2  David and Victoria Beckham finally addressing ...    800   \n",
       "3  Victoria Beckham shares photo of husband David...   2687   \n",
       "4                    Micah Richards on David Beckham   5650   \n",
       "\n",
       "                  date                                   url  num_comments  \\\n",
       "0  2023-10-06 17:16:05       https://v.redd.it/0mmzjpgdyisb1          2042   \n",
       "1  2024-01-02 14:44:45  https://i.redd.it/dowdawvo7y9c1.jpeg           982   \n",
       "2  2023-10-05 20:46:41       https://v.redd.it/ahnysog1vcsb1           261   \n",
       "3  2023-12-16 10:45:59  https://i.redd.it/nd35a08lpj6c1.jpeg           298   \n",
       "4  2024-09-19 18:27:36       https://v.redd.it/7o6n7df28qpd1           256   \n",
       "\n",
       "                                            comments  \n",
       "0  [Please report rule breaking posts, such as:\\n...  \n",
       "1  [Welcome to r/popculturechat! ☺️\\n\\nAs a proud...  \n",
       "2  [I think if they hadn’t brought this up in a d...  \n",
       "3  [Welcome to r/popculturechat! ☺️\\n\\nAs a proud...  \n",
       "4  [**Mirrors / Alternative Angles**\\n  \\n\\n*I am...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('DB_reddit.json', 'r', encoding='utf-8') as file:\n",
    "    reddit = json.load(file)\n",
    "\n",
    "# Converting the data into a DataFrame \n",
    "df = pd.DataFrame(reddit)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7da3c",
   "metadata": {},
   "source": [
    "#### Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e4d1176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      "title           0\n",
      "score           0\n",
      "date            0\n",
      "url             0\n",
      "num_comments    0\n",
      "comments        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values in the dataset\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Display missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f128a6",
   "metadata": {},
   "source": [
    "#### Check for Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45a70168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "duplicates = df.duplicated(subset=['url']).sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "df.drop_duplicates(subset=['url'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22975dc",
   "metadata": {},
   "source": [
    "#### Advance Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dea7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Removing URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Removing special characters \n",
    "    text = re.sub(r\"[^a-zA-Z0-9'\\s]\", '', text)\n",
    "    # Converting to lowercase\n",
    "    text = text.lower()\n",
    "    # Removing extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Function to detect if the text is in English\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8d54653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and preprocess titles\n",
    "df['Cleaned_Title'] = df['title'].apply(lambda x: expand_contractions(clean_text(str(x))) if is_english(str(x)) else '')\n",
    "\n",
    "# Preprocessing each comment within the list\n",
    "df['Cleaned_Comments'] = df['comments'].apply(lambda comments: [expand_contractions(clean_text(comment)) for comment in comments if is_english(comment)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c4fef",
   "metadata": {},
   "source": [
    "#### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4b15756",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def process_tokens(tokens):\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # If no tokens are left after stopword removal, return the original tokens\n",
    "    if not tokens:\n",
    "        return tokens\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21aee7f",
   "metadata": {},
   "source": [
    "#### Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aff87ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and lemmatize the cleaned text\n",
    "df['Title_Tokens'] = df['Cleaned_Title'].apply(lambda x: process_tokens(word_tokenize(x)))\n",
    "df['Comments_Tokens'] = df['Cleaned_Comments'].apply(lambda comments: [process_tokens(word_tokenize(comment)) for comment in comments])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6c97c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Data:\n",
      "                                               title  \\\n",
      "0                     Not under David Beckhams watch   \n",
      "1            David Beckham is not letting this go...   \n",
      "2  David and Victoria Beckham finally addressing ...   \n",
      "3  Victoria Beckham shares photo of husband David...   \n",
      "4                    Micah Richards on David Beckham   \n",
      "5                                Young David Beckham   \n",
      "6                         Thoughts on David Beckham?   \n",
      "7  david beckham: then vs now – is it just me or ...   \n",
      "8                          David Beckham is autistic   \n",
      "9  David and Victoria Beckham recreate their wedd...   \n",
      "\n",
      "                                       Cleaned_Title  \\\n",
      "0                     not under david beckhams watch   \n",
      "1               david beckham is not letting this go   \n",
      "2  david and victoria beckham finally addressing ...   \n",
      "3  victoria beckham shares photo of husband david...   \n",
      "4                    micah richards on david beckham   \n",
      "5                                                      \n",
      "6                          thoughts on david beckham   \n",
      "7  david beckham then vs now is it just me or doe...   \n",
      "8                          david beckham is autistic   \n",
      "9  david and victoria beckham recreate their wedd...   \n",
      "\n",
      "                                        Title_Tokens  \\\n",
      "0                           [david, beckhams, watch]   \n",
      "1                      [david, beckham, letting, go]   \n",
      "2  [david, victoria, beckham, finally, addressing...   \n",
      "3  [victoria, beckham, share, photo, husband, dav...   \n",
      "4                  [micah, richards, david, beckham]   \n",
      "5                                                 []   \n",
      "6                          [thought, david, beckham]   \n",
      "7            [david, beckham, v, longer, look, type]   \n",
      "8                         [david, beckham, autistic]   \n",
      "9  [david, victoria, beckham, recreate, wedding, ...   \n",
      "\n",
      "                                            comments  \\\n",
      "0  [Please report rule breaking posts, such as:\\n...   \n",
      "1  [Welcome to r/popculturechat! ☺️\\n\\nAs a proud...   \n",
      "2  [I think if they hadn’t brought this up in a d...   \n",
      "3  [Welcome to r/popculturechat! ☺️\\n\\nAs a proud...   \n",
      "4  [**Mirrors / Alternative Angles**\\n  \\n\\n*I am...   \n",
      "5  [He was fine as hell in his prime, He was sooo...   \n",
      "6  [The definition of a silver fox. Zaddy as hell...   \n",
      "7  [Just a note that neither Victoria nor David B...   \n",
      "8  [Neuro Spice was definitly the least understoo...   \n",
      "9  [Welcome to r/popculturechat! ☺️\\n\\nAs a proud...   \n",
      "\n",
      "                                    Cleaned_Comments  \\\n",
      "0  [please report rule breaking posts such as pol...   \n",
      "1  [welcome to rpopculturechat as a proud bipoc l...   \n",
      "2  [i think if they had not brought this up in a ...   \n",
      "3  [welcome to rpopculturechat as a proud bipoc l...   \n",
      "4  [mirrors alternative angles i am a bot and thi...   \n",
      "5  [he was fine as hell in his prime, he was sooo...   \n",
      "6  [the definition of a silver fox zaddy as hell,...   \n",
      "7  [just a note that neither victoria nor david b...   \n",
      "8  [neuro spice was definitly the least understoo...   \n",
      "9  [welcome to rpopculturechat as a proud bipoc l...   \n",
      "\n",
      "                                     Comments_Tokens  \n",
      "0  [[please, report, rule, breaking, post, politi...  \n",
      "1  [[welcome, rpopculturechat, proud, bipoc, lgbt...  \n",
      "2  [[think, brought, doc, specifically, covering,...  \n",
      "3  [[welcome, rpopculturechat, proud, bipoc, lgbt...  \n",
      "4  [[mirror, alternative, angle, bot, action, per...  \n",
      "5  [[fine, hell, prime], [sooo, hot, hair, 5, bes...  \n",
      "6  [[definition, silver, fox, zaddy, hell], [nice...  \n",
      "7  [[note, neither, victoria, david, beckham, ver...  \n",
      "8  [[neuro, spice, definitly, least, understood, ...  \n",
      "9  [[welcome, rpopculturechat, proud, bipoc, lgbt...  \n"
     ]
    }
   ],
   "source": [
    "# Display the processed data\n",
    "processed_columns = ['title', 'Cleaned_Title', 'Title_Tokens', 'comments', 'Cleaned_Comments', 'Comments_Tokens']\n",
    "print(\"\\nProcessed Data:\")\n",
    "print(df[processed_columns].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ed9a7",
   "metadata": {},
   "source": [
    "#### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5df537cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 most common terms:\n",
      "like: 2625\n",
      "would: 1827\n",
      "people: 1737\n",
      "think: 1395\n",
      "one: 1358\n",
      "know: 1190\n",
      "look: 1129\n",
      "class: 1110\n",
      "get: 1088\n",
      "beckham: 1078\n",
      "time: 1000\n",
      "really: 913\n",
      "even: 912\n",
      "david: 853\n",
      "good: 841\n",
      "also: 822\n",
      "make: 805\n",
      "year: 798\n",
      "thing: 780\n",
      "much: 771\n",
      "love: 720\n",
      "still: 701\n",
      "way: 697\n",
      "'s: 677\n",
      "never: 670\n",
      "could: 669\n",
      "see: 623\n",
      "working: 618\n",
      "well: 613\n",
      "going: 604\n",
      "money: 598\n",
      "say: 581\n",
      "lol: 564\n",
      "always: 564\n",
      "got: 561\n",
      "go: 550\n",
      "lot: 542\n",
      "want: 530\n",
      "right: 517\n",
      "victoria: 511\n",
      "work: 510\n",
      "someone: 507\n",
      "mean: 505\n",
      "u: 503\n",
      "said: 460\n",
      "man: 459\n",
      "woman: 459\n",
      "back: 454\n",
      "made: 448\n",
      "though: 444\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "def process_text(text, lemmatizer, stopwords_set):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords_set]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "term_freq_counter = Counter()\n",
    "\n",
    "# Processing each row in the DataFrame and updating term frequency counts\n",
    "for _, row in df.iterrows(): \n",
    "    tokens_title = process_text(row['Cleaned_Title'], lemmatizer, stopwords_set)\n",
    "    tokens_comments = [word for comment in row['Cleaned_Comments'] for word in process_text(comment, lemmatizer, stopwords_set)]\n",
    "\n",
    "    # Updating term frequency counts\n",
    "    term_freq_counter.update(tokens_title)\n",
    "    term_freq_counter.update(tokens_comments)\n",
    "\n",
    "# Displaying the most common terms\n",
    "freq_num = 50  \n",
    "print(f\"Top {freq_num} most common terms:\")\n",
    "for term, count in term_freq_counter.most_common(freq_num):\n",
    "    print(f\"{term}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d91a110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
